{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c351ec1",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">MountainCar and MountainCarCOntinuous Enviroment with PPO </span>\n",
    "\n",
    "by Robin Wolf and Mathias Fuhrer (RKIM)\n",
    "Project: Reinforcement Learning in module 'Roboterprogrammierung' by Prof. Hein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd309a6",
   "metadata": {},
   "source": [
    "This notebook contains all of our analysis and improvement process of the given PPO implementation on open ai's MountainCar and MountainCarContinuous enviroment. A detailed comparison in respect to implementation details and agent performance between stable baselines PPO and our PPO is included too.\n",
    "\n",
    "### Table of Content:\n",
    "1) Proximal Policy Optimization (PPO)\n",
    "2) Difference between discrete and continuous action space implementation\n",
    "3) MountainCar and MountainCarContinuous Environment\n",
    "4) Reward shaping\n",
    "5) Training with our implementation\n",
    "6) Results\n",
    "7) Comparison to stable baseline3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318574f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9daa54a",
   "metadata": {},
   "source": [
    "### 1) Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db9ccbb",
   "metadata": {},
   "source": [
    "\n",
    "PPO is a policy gradient Actor-Critic algorithm. The policy model, the **actor** network  produces a stochastic policy. It maps the state to a probability distribution over the set of possible actions. The **critic** network is used to approximate the value function and then, the advantage is calculated:\n",
    "\n",
    "$$\n",
    "A_\\Phi (s_t, a_t) = q_\\Phi (s_t,a_t) - v_\\Phi (s_t) = R_t + \\gamma v_{\\Phi'} (s_{t+1}) - v_\\Phi (s_t)\n",
    "$$\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$\n",
    "\n",
    "PPO uses 2 main models. The actor network learns the stochastic policy. It maps the state to a probability distribution over the set of possible actions. The critic network learns the value function. It maps the state to a scalar.\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b4f99",
   "metadata": {},
   "source": [
    "<img src=\"visu/PPO.png\" alt=\"PPO-Shematic\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1527f6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d389eeac",
   "metadata": {},
   "source": [
    "### 2) Difference between discrete and continuous action space implementation\n",
    "In the discrete case, the actor network has an output for every possible action with a probability for this action in the given state.\n",
    "In the Mountain Car Environment there are 3 possible actions: Accelerate to the left, to the right or not accelerate at all.\n",
    "\n",
    "<img src=\"visu/actionsDiscrete.png\" alt=\"actionsDiscrete\" width=\"500\"/>\n",
    "\n",
    "In a continuous action space you need a probability distribution for each degree of freedom. In the case of the Mountain Car Environment, the acceleration to the left or right would be one degree of freedom. In the Hopper Environment there are 3 degrees of freedom for the torques at the 3 joints (see other notebook)\n",
    "With a continuous action space, the actor network must output a probability distribution consisting of the mean and standard deviation for each degree of freedom.\n",
    "\n",
    "<img src=\"visu/actionsContinuous.png\" alt=\"actionsContinuous\" width=\"500\"/>\n",
    "\n",
    "The actor network, the act and the get_actor_grads methods of the PPO agent must be adjusted accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32cf85",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efd7540",
   "metadata": {},
   "source": [
    "### 3) MountainCar and MountainCarContinuous Environment\n",
    "### 3.1) MountainCar\n",
    "https://gymnasium.farama.org/environments/classic_control/mountain_car/  \n",
    " \n",
    "Length of one episode is 200 steps \n",
    "\n",
    "**Observation Space (2 dimensional):**\n",
    "- position of the car along the x-axis [-1,2 … 0,6]\n",
    "- velocity of the car [-0,7 … 0,7]\n",
    "\n",
    "**Action Space (3 dimensional):** \n",
    "- 0: Accelerate to the left\n",
    "- 1: Don’t accelerate\n",
    "- 2: Accelerate to the right\n",
    "\n",
    "**Reward:** \n",
    " - negative reward of -1 at each timestep\n",
    " \n",
    "  \n",
    "   \n",
    "### 3.2) MountainCarContinuous\n",
    "https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/ \n",
    "  \n",
    "Length of one episode is 999 steps\n",
    "   \n",
    "**Observation Space (2 dimensional):** \n",
    "- position of the car along the x-axis [-Inf … Inf]\n",
    "- velocity of the car [-Inf … Inf]\n",
    "**Action Space (1 dimensional):**\n",
    "- force applied to the car [-1 … 1]\n",
    "\n",
    "**Reward:** \n",
    "- negative reward of -0.1 * action2 at each timestep, positive reward of +100 added if the car reaches the goal\n",
    "\n",
    "<img src=\"visu/MtnCar.png\" alt=\"MtnCar\" width=\"400\"/> \n",
    "\n",
    "\n",
    "**--> The goal is to reach the flag on the right hill**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fc7a8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1fd7df",
   "metadata": {},
   "source": [
    "### 4) Reward shaping\n",
    "In the discrete environment, the agent receives a negative reward of -1 at each time step. If the vehicle doesn't make it to the finish, this adds up to a return of -200 in each episode. Only if the vehicle happens to make it to the finish line and the episode ends earlier the return will be greater than -200. The agent cannot learn anything useful if the vehicle does not make it to the destination.\n",
    "\n",
    "In the continuous environment, the agent receives a negative reward at each time step, which increases as the height of the action increases. If the vehicle makes it to the finish line, it will also receive a positive reward of +100. If the vehicle doesn't make it to the finish line by chance, the agent tends to learn to do nothing or as little as possible in order to keep the negative reward as low as possible.\n",
    "\n",
    "To solve this, we changed the reward strategy of the original environments. This is quite common and is called reward shaping.\n",
    "\n",
    "\n",
    "We tried two different things.\n",
    "\n",
    "On the first attempt, we gave him a staggered positive reward as he made his way up the right hill bit by bit. The idea was that he would learn to go up the right hill.\n",
    "\n",
    "<img src=\"visu/rewardPosition.png\" alt=\"MtnCar\" width=\"400\"/> \n",
    "\n",
    "\n",
    "The second consideration was that he gets a positive reward if the vehicle is fast. The idea was that if the vehicle was fast, it would inevitably have to get up the hill.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a32c91",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc86db",
   "metadata": {},
   "source": [
    "### 5) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f59364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorboard\n",
    "from keras.callbacks import TensorBoard  # to visualize the training process\n",
    "import os\n",
    "import datetime\n",
    "import pygame\n",
    "import mujoco\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe3c8ca",
   "metadata": {},
   "source": [
    "<u> Parameter </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90aca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter for the actor and critic networks\n",
    "actor_learning_rate = 0.00025   # learning rate for the actor\n",
    "critic_learning_rate = 0.001    # learning rate for the critic\n",
    "\n",
    "# Parameter for the agent\n",
    "gamma = 0.99                    # discount factor\n",
    "epsilon = 0.1                   # clip range for the actor loss function\n",
    "\n",
    "# Parameter for training\n",
    "epochs = 1#50                  # number of learning iterations\n",
    "n_rollouts = 5                  # number of episodes/ rollouts to collect experience\n",
    "batch_size = 8                  # number of samples per learning step\n",
    "learn_steps = 16                # number of learning steps per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80120468",
   "metadata": {},
   "source": [
    "### 5.1) discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7b9e0",
   "metadata": {},
   "source": [
    "<u> Initializing Datalogging </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae1fa298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory model\\CustomMountainCarEnv_velocity_discrete_20240115_202429\\log exists.\n",
      "c:\\Users\\Mathias\\Documents\\StudiumMaster\\Semester1\\Roboterprogrammierung_Hein\\Projektarbeit_PPO\\04_Abgabe\\01_Code\\model\\CustomMountainCarEnv_velocity_discrete_20240115_202429\\log\n"
     ]
    }
   ],
   "source": [
    "# refers to log data and model data -> below for model data\n",
    "jetzt1 = datetime.datetime.now()\n",
    "datum_uhrzeit1 = jetzt1.strftime(\"%Y%m%d_%H%M%S\")\n",
    "savedir1 = f'model\\\\CustomMountainCarEnv_velocity_discrete_{datum_uhrzeit1}'\n",
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs(savedir1, exist_ok=True)\n",
    "log_dir1 = f\"{savedir1}\\\\log\"\n",
    "os.makedirs(log_dir1, exist_ok=True)\n",
    "\n",
    "if os.path.exists(log_dir1):\n",
    "    print(f\"The directory {log_dir1} exists.\")\n",
    "    absolute_path1 = os.path.abspath(log_dir1)\n",
    "    print(absolute_path1)\n",
    "else:\n",
    "    print(f\"The directory {log_dir1} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545f84b",
   "metadata": {},
   "source": [
    "<u> Environment initialisieren </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62f092ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.CustomMtnCarEnvironments import CustomMountainCarEnv_velocity\n",
    "from lib.CustomMtnCarEnvironments import CustomMountainCarEnv_position\n",
    "\n",
    "env1 = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "\n",
    "# Reward shaping\n",
    "env1 = CustomMountainCarEnv_velocity(env1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adfb175",
   "metadata": {},
   "source": [
    "<u> initializing PPO-Agent </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7554d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.PPOAgentDiscrete import PPOAgentDiscrete\n",
    "agent1 = PPOAgentDiscrete(env1.action_space, env1.observation_space, gamma, epsilon, actor_learning_rate, critic_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a30734",
   "metadata": {},
   "source": [
    "<u> training PPO-Agent</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c191d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, learn step 0 of 16 finished --> actor loss 0.02296990156173706, critic loss 0.43175390362739563\n",
      "update online nets, learn step 1 of 16 finished --> actor loss -0.014846965670585632, critic loss 0.0029164645820856094\n",
      "update online nets, learn step 2 of 16 finished --> actor loss -0.006653338670730591, critic loss 0.00811038352549076\n",
      "update online nets, learn step 3 of 16 finished --> actor loss -0.02178068459033966, critic loss 0.03970930725336075\n",
      "update online nets, learn step 4 of 16 finished --> actor loss 0.030912980437278748, critic loss 0.8937200307846069\n",
      "update online nets, learn step 5 of 16 finished --> actor loss 0.029167860746383667, critic loss 0.0022610872983932495\n",
      "update online nets, learn step 6 of 16 finished --> actor loss -0.0076074860990047455, critic loss 0.0006917249993421137\n",
      "update online nets, learn step 7 of 16 finished --> actor loss -0.030809350311756134, critic loss 0.0006028945208527148\n",
      "update online nets, learn step 8 of 16 finished --> actor loss 0.006258666515350342, critic loss 0.002587243914604187\n",
      "update online nets, learn step 9 of 16 finished --> actor loss -0.0029627978801727295, critic loss 0.00034819659776985645\n",
      "update online nets, learn step 10 of 16 finished --> actor loss -0.0013883411884307861, critic loss 0.00029259303119033575\n",
      "update online nets, learn step 11 of 16 finished --> actor loss 0.0026906579732894897, critic loss 0.021645892411470413\n",
      "update online nets, learn step 12 of 16 finished --> actor loss -0.0024267733097076416, critic loss 0.05421827360987663\n",
      "update online nets, learn step 13 of 16 finished --> actor loss -0.000138845294713974, critic loss 0.0008792071603238583\n",
      "update online nets, learn step 14 of 16 finished --> actor loss 0.005872875452041626, critic loss 0.00222960626706481\n",
      "update online nets, learn step 15 of 16 finished --> actor loss 0.01804618537425995, critic loss 2.683047294616699\n",
      "update frozen nets, epoche 0 of 1 finished\n",
      "===> epoch 1, total_timesteps 1000, actor loss 0.01804618537425995, critic loss 2.683047294616699, avg_epoch_return -4.263256414560601e-15, sum_epoch_terminations 0\n"
     ]
    }
   ],
   "source": [
    "from lib.train_agent import training_rollouts as training\n",
    "training(env1, agent1, log_dir1, epochs, n_rollouts, batch_size, learn_steps, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aca0e0",
   "metadata": {},
   "source": [
    "<u> Storing models </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f424c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to h5 format\n",
    "filepath_actor1 = f\"{savedir1}\\\\actor.h5\"\n",
    "filepath_critic1 = f\"{savedir1}\\\\critic.h5\"\n",
    "agent1.save_models(filepath_actor1, filepath_critic1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d505ac7",
   "metadata": {},
   "source": [
    "<u> Rendering with pygame </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "698cbb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded sucessful\n",
      "Episode 0 finished\n",
      "Closed Rendering sucessful\n"
     ]
    }
   ],
   "source": [
    "from lib.render_GUI import render_GUI\n",
    "\n",
    "\n",
    "# Set up the enviroment and load the trained agent from directory\n",
    "render_env1 = gym.make('MountainCar-v0', render_mode = 'human')\n",
    "\n",
    "render_agent1 = PPOAgentDiscrete(render_env1.action_space, render_env1.observation_space)\n",
    "render_agent1._init_networks()\n",
    "\n",
    "# filepath_actor1 = f\"model\\\\ TODO \\\\actor.h5\"\n",
    "# filepath_critic1 = f\"model\\\\ TODO \\\\critic.h5\"\n",
    "\n",
    "# load the model from h5 format\n",
    "render_agent1.load_models(filepath_actor1, filepath_critic1)\n",
    "\n",
    "#call the function\n",
    "render_GUI(render_env1, render_agent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd0ec7d",
   "metadata": {},
   "source": [
    "### 5.1) continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e6c19",
   "metadata": {},
   "source": [
    "<u> Initializing Datalogging </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "533c46d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory model\\CustomMountainCarEnv_velocity_continuous_20240115_202950\\log exists.\n",
      "c:\\Users\\Mathias\\Documents\\StudiumMaster\\Semester1\\Roboterprogrammierung_Hein\\Projektarbeit_PPO\\04_Abgabe\\01_Code\\model\\CustomMountainCarEnv_velocity_continuous_20240115_202950\\log\n"
     ]
    }
   ],
   "source": [
    "# refers to log data and model data -> below for model data\n",
    "jetzt2 = datetime.datetime.now()\n",
    "datum_uhrzeit2 = jetzt2.strftime(\"%Y%m%d_%H%M%S\")\n",
    "savedir2 = f'model\\\\CustomMountainCarEnv_velocity_continuous_{datum_uhrzeit2}'\n",
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs(savedir2, exist_ok=True)\n",
    "\n",
    "\n",
    "#from Fctn_log_metrics import log_metrics\n",
    "log_dir2 = f\"{savedir2}\\\\log\"\n",
    "os.makedirs(log_dir2, exist_ok=True)\n",
    "\n",
    "if os.path.exists(log_dir2):\n",
    "    print(f\"The directory {log_dir2} exists.\")\n",
    "    absolute_path2 = os.path.abspath(log_dir2)\n",
    "    print(absolute_path2)\n",
    "else:\n",
    "    print(f\"The directory {log_dir2} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb89c189",
   "metadata": {},
   "source": [
    "<u> Initializing environment</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe517ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.CustomMtnCarEnvironments import CustomMountainCarEnv_velocity\n",
    "from lib.CustomMtnCarEnvironments import CustomMountainCarEnv_position\n",
    "\n",
    "env2 = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
    "\n",
    "# Reward shaping\n",
    "env2 = CustomMountainCarEnv_velocity(env2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bfdbc5",
   "metadata": {},
   "source": [
    "<u> Initializing PPO-Agent </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84f32a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.PPOAgentContinuous import PPOAgentContinuous\n",
    "agent2 = PPOAgentContinuous(env2.action_space, env2.observation_space, gamma, epsilon, actor_learning_rate, critic_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0228c",
   "metadata": {},
   "source": [
    "<u> Training PPO-Agent </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6235990d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, learn step 0 of 16 finished --> actor loss -0.00444340705871582, critic loss 0.15101514756679535\n",
      "update online nets, learn step 1 of 16 finished --> actor loss -0.002040386199951172, critic loss 0.0020012622699141502\n",
      "update online nets, learn step 2 of 16 finished --> actor loss -0.01733473315834999, critic loss 0.00512193376198411\n",
      "update online nets, learn step 3 of 16 finished --> actor loss -0.003203696571290493, critic loss 0.032389018684625626\n",
      "update online nets, learn step 4 of 16 finished --> actor loss 0.012715565040707588, critic loss 0.012662029825150967\n",
      "update online nets, learn step 5 of 16 finished --> actor loss -0.006671170238405466, critic loss 0.00016426289221271873\n",
      "update online nets, learn step 6 of 16 finished --> actor loss -0.005980385467410088, critic loss 0.11311068385839462\n",
      "update online nets, learn step 7 of 16 finished --> actor loss -0.00513810571283102, critic loss 0.08736348897218704\n",
      "update online nets, learn step 8 of 16 finished --> actor loss -0.03427376225590706, critic loss 0.45797815918922424\n",
      "update online nets, learn step 9 of 16 finished --> actor loss -0.001023199874907732, critic loss 0.004125072155147791\n",
      "update online nets, learn step 10 of 16 finished --> actor loss 0.008201731368899345, critic loss 0.004856939427554607\n",
      "update online nets, learn step 11 of 16 finished --> actor loss 0.0005133152008056641, critic loss 0.0011623307364061475\n",
      "update online nets, learn step 12 of 16 finished --> actor loss -0.00621993001550436, critic loss 0.0372687391936779\n",
      "update online nets, learn step 13 of 16 finished --> actor loss -0.007461667060852051, critic loss 0.0012841495918110013\n",
      "update online nets, learn step 14 of 16 finished --> actor loss -0.0008293125429190695, critic loss 0.001530140289105475\n",
      "update online nets, learn step 15 of 16 finished --> actor loss 0.009816858917474747, critic loss 0.0007589645683765411\n",
      "update frozen nets, epoche 0 of 1 finished\n",
      "===> epoch 1, total_timesteps 4995, actor loss 0.009816858917474747, critic loss 0.0007589645683765411, avg_epoch_return -4.5474735088646414e-14, sum_epoch_terminations 0\n"
     ]
    }
   ],
   "source": [
    "from lib.train_agent import training_rollouts as training\n",
    "training(env2, agent2, log_dir2, epochs, n_rollouts, batch_size, learn_steps, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9265b9",
   "metadata": {},
   "source": [
    "<u> Storing models </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fc77198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to h5 format\n",
    "filepath_actor2 = f\"{savedir2}\\\\actor.h5\"\n",
    "filepath_critic2 = f\"{savedir2}\\\\critic.h5\"\n",
    "agent2.save_models(filepath_actor2, filepath_critic2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0547ec",
   "metadata": {},
   "source": [
    "<u> Rendering with pygame </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5715e588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model reloaded sucessful\n",
      "Closed Rendering sucessful\n"
     ]
    }
   ],
   "source": [
    "from lib.render_GUI import render_GUI\n",
    "\n",
    "\n",
    "# Set up the enviroment and load the trained agent from directory\n",
    "render_env2 = gym.make('MountainCarContinuous-v0', render_mode = 'human')\n",
    "render_agent2 = PPOAgentContinuous(render_env2.action_space, render_env2.observation_space)\n",
    "render_agent2._init_networks()\n",
    "\n",
    "# filepath_actor2 = f\"model\\\\ TODO \\\\actor.h5\"\n",
    "# filepath_critic2 = f\"model\\\\ TODO \\\\critic.h5\"\n",
    "\n",
    "# load the model from h5 format\n",
    "render_agent2.load_models(filepath_actor2, filepath_critic2)\n",
    "\n",
    "#call the function\n",
    "render_GUI(render_env2, render_agent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21df72f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688468c5",
   "metadata": {},
   "source": [
    "### 6) Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df96f39",
   "metadata": {},
   "source": [
    "\n",
    "**Position based reward** \n",
    "  \n",
    "As you can see in the video, the position based approach was not successful. The vehicle tries to drive up the right hill without gaining the necessary momentum on the left hill.\n",
    "If the vehicle were to gain momentum on the left, it would receive a worse reward in the meantime. This causes the agent to get stuck in a local maximum.\n",
    " \n",
    "<video width=\"400\" height=\"400\" controls>\n",
    "  <source src=\"visu/CustomMountainCarEnv_position_discrete_20240111_174107.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "**Velocity based reward** \n",
    " \n",
    "The velocity based approach was significantly more successful. For both discrete and continuous environments.\n",
    "In the diagrams you can see that the average return and the terminations converge well. Termination here means that the episode ended early because the vehicle made it to the finish line. You can see here that at the end the vehicle has reached the destination in each of the 5 episodes per epoch.\n",
    " \n",
    "<video width=\"400\" height=\"400\" controls>\n",
    "  <source src=\"visu/CustomMountainCarEnv_velocity_discrete_20240114_000941_200epochs.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "Learning curve (x-axis = epochs/ y-axis = average return):  \n",
    "<img src=\"visu/mtnCar_avgReturn.png\" alt=\"mtnCar_avgReturn\" width=\"400\"/>\n",
    "\n",
    "Learning curve (x-axis = epochs/ y-axis = terminations per epoch (here one epoch has 5 episodes)):   \n",
    "<img src=\"visu/mtnCar_avgReturn.png\" alt=\"mtnCar_avgReturn\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce64238",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80751157",
   "metadata": {},
   "source": [
    "### 7) Comparison to stable baseline3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46731adb",
   "metadata": {},
   "source": [
    "<u> Initializing and Training an agent with SB3 </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stable baselines algorithms (installation of the package recommended)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937ab097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory (./baselines_model)to save all trained agents with a timestamp\n",
    "jetzt = datetime.datetime.now()\n",
    "datum_uhrzeit = jetzt.strftime(\"%Y%m%d_%H%M%S\")\n",
    "savedir = f'baselines_model\\\\YOUR_AGENT_NAME_HERE_{datum_uhrzeit}'\n",
    "os.makedirs('baselines_model', exist_ok=True)\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "# create a directory to save logs from Tensorboard (Visualization and analysis tool from Tensorflow)\n",
    "log_dir = f\"{savedir}\\\\log\"\n",
    "os.makedirs(log_dir1, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3395a1f",
   "metadata": {},
   "source": [
    "Attention: Run only one from the next two cells, the init-process is different for enviroments in preferred SB3 framework and wrapped enviroments in the GYM framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd0c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the agent (in stable baselines3 using a vectorized enviroment is prefered)\n",
    "# --> SB3 framework, not compatible with GYM framework\n",
    "vec_env = make_vec_env('Hopper-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the agent with a wrapped enviroment (now the GYM-Framework must be used!)\n",
    "inheritance_env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
    "\n",
    "# Reward shaping (Velocity Reward as an example)\n",
    "env = CustomMountainCarEnv_velocity(inheritance_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the agent in baselines framework (used policy, enviroment, optional: log)\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log= log_dir)   \n",
    "# this model provides continuous feedback about the training process when verbose 1 (doesn't effects runtime that much)\n",
    "\n",
    "# call the learn method (training ends, if agent passes 1 Mio timesteps ini total)\n",
    "model.learn(total_timesteps=1000000) \n",
    "\n",
    "# save the model (this will create a data.zip folder containing all model informations)\n",
    "model.save(savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9bf127",
   "metadata": {},
   "source": [
    "<u> Reloading and Rendering </u>\n",
    "\n",
    "Choose one of both rendering frameworks in respect to the enviroment you trained the agent in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the rendering function (self-written) for the sb3 framework\n",
    "from lib.render_GUI import render_GUI_SB3\n",
    "\n",
    "# define the directory the model is loaded from\n",
    "loaddir = 'baseline_model\\YOUR_AGENT_NAME_HERE\\data.zip'\n",
    "\n",
    "# load the model with sb3 function\n",
    "render_agent = PPO.load(loaddir)\n",
    "\n",
    "# initialize a enviroment (must be the same framework as used while training)\n",
    "render_env = make_vec_env('MountainCarContinuous-v0')\n",
    "\n",
    "# call the render function \n",
    "# Note: to close this window you have to interrupt this running cell, closing the window with the red x doesn't work\n",
    "render_GUI_SB3(render_env, render_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b139f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the rendering function (self-written) for the GYM framework (wrapped enviroments)\n",
    "from lib.render_GUI import render_GUI_GYM\n",
    "\n",
    "# define the directory the model is loaded from\n",
    "loaddir = 'baseline_model\\YOUR_AGENT_NAME_HERE\\data.zip'\n",
    "\n",
    "# load the model with sb3 function\n",
    "render_agent = PPO.load(loaddir)\n",
    "\n",
    "# initialize a enviroment (must be the same framework as used while training)\n",
    "render_env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "# call the render function \n",
    "# Note: to close this window you have to interrupt this running cell, closing the window with the red x doesn't work\n",
    "render_GUI_GYM(render_env, render_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ae679",
   "metadata": {},
   "source": [
    "<u> Comparison of trained agents in standard enviroment </u>\n",
    "\n",
    "Trained discrete agent with default hyperparameters from the BaselinesZoo by DLR in the standard env:      \n",
    "<video width=\"200\" height=\"200\" controls>\n",
    "  <source src=\"visu/SB3-dis-standard.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "Learning curve (x-axis = total timesteps/ y-axis = return):  \n",
    "<img src=\"visu/LC-discrete-standard.jpg\" alt=\"Learning Curve SB3 discrete MntnCar\" width=\"400\"/>\n",
    "\n",
    "- This approach works well if the goal at the top of the hill is reached once (high slope around 500.000 timesteps)\n",
    "- It's more or less randomly when the agent hit's the goal first during exploration\n",
    "- in comparison to our implementation, the agent from SB3 solves the enviroment without reward shaping too.\n",
    "\n",
    "\n",
    "Trained continous agent with default hyperparameters from the BaselinesZoo by DLR in the standard env:      \n",
    "<video width=\"200\" height=\"200\" controls>\n",
    "  <source src=\"visu/SB3-cont-standard.mp4\" type=\"video/mp4\"> \n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "Learning curve (x-axis = total timesteps/ y-axis = return):  \n",
    "<img src=\"visu/LC-cont-standard.jpg\" alt=\"Learning Curve SB3 continuous MntnCar\" width=\"400\"/>\n",
    "\n",
    "- This approach doesn't solve the enviroment.\n",
    "- Agent learns to do 'nothing' due to big actions are penalized ba a negative reward\n",
    "- Agent doesn't explore the positive reward if he terimnates\n",
    "- Learning progress get stuck at a local loss minimum/ return maximum at 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11267b6",
   "metadata": {},
   "source": [
    "<u> Comparison of discrete agents in wrapped enviroments: </u>\n",
    "\n",
    "Trained discrete agent in the wrapped enviroment (position reward shaping) env:      \n",
    "<video width=\"200\" height=\"200\" controls>\n",
    "  <source src=\"visu/SB3-dis-position.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "Learning curve (x-axis = total timesteps/ y-axis = return):  \n",
    "<img src=\"visu/LC-discrete-position.jpg\" alt=\"Learning Curve SB3 MC_position\" width=\"400\"/>\n",
    "\n",
    "- the agent does't terminate/ enviroment is not solved\n",
    "- because there's only a positive reward on the righthand side of the hill, the agent doesn't learn to get momentum by climbing the lefthand side hill.\n",
    "- Same dynamics/ behavior as recognized in our implementation.\n",
    "\n",
    "Trained discrete agent in the wrapped enviroment (velocity reward shaping) env:     \n",
    "<video width=\"200\" height=\"200\" controls>\n",
    "  <source src=\"visu/SB3-dis-velocity.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "Learning curve (x-axis = total timesteps/ y-axis = return):  \n",
    "<img src=\"visu/LC-discrete-velocity.jpg\" alt=\"Learning Curve SB3 MC velocity\" width=\"400\"/>\n",
    "\n",
    "- agent solves the wrapped enviroment with shaped reward well\n",
    "- terminates much earlier in comparison to the standard enviroment\n",
    "- performance (max. return) nearly equal, but learning is a more stable or deterministic process\n",
    "- in comparison to our  implementation we have to mention, that SB3 is much less sample efficient (we need approx. 300.000 - 400.000 timesteps to solve the enviroment in comparison to <10.000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7c9ef6",
   "metadata": {},
   "source": [
    "<u> Comparison of continuous agents in wrapped enviroments: </u>\n",
    "\n",
    "Trained continuous agent in the wrapped enviroment (position reward shaping) env:      \n",
    "<video width=\"200\" height=\"200\" controls>\n",
    "  <source src=\"visu/SB3-cont-position.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "Learning curve (x-axis = total timesteps/ y-axis = return):  \n",
    "<img src=\"visu/LC-cont-position.jpg\" alt=\"Learning Curve SB3 MCC_position\" width=\"400\"/>\n",
    "\n",
    "- enviroment is not solved\n",
    "- Same dynamics/ behavior as recognized in our implementation and with discrete PPO\n",
    "- actions taken by the continuous agent are smaller than the discrete ones, because they are penalized by a negative reward\n",
    "\n",
    "Trained discrete agent in the wrapped enviroment (velocity reward shaping) env:     \n",
    "<video width=\"200\" height=\"200\" controls>\n",
    "  <source src=\"visu/SB3-cont-velocity.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "Learning curve (x-axis = total timesteps/ y-axis = return):  \n",
    "<img src=\"visu/LC-cont-velocity.jpg\" alt=\"Learning Curve SB3 MCC_velocity\" width=\"400\"/>\n",
    "\n",
    "- agent solves the wrapped enviroment with shaped reward well\n",
    "- needs more attempts to build up enough momentum to reach the goal\n",
    "- that's because of the balance between a reward penalizing big actions and a other reward pushing the agent to geh high velocities (they work against each other)\n",
    "- in comparison to our implementation, this agent needs much more timesteps to learn terminating (same as discrete agent with SB3 -> a educated guess why is described in the Hopper-Notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
