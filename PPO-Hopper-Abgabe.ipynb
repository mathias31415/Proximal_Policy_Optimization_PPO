{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Hopper Enviroment with PPO </span>\n",
    "\n",
    "by Robin Wolf and Mathias Fuhrer (RKIM)\n",
    "Project: Reinforcement Learning in module 'Roboterprogrammierung' by Prof. Hein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "import tensorboard\n",
    "from keras.callbacks import TensorBoard\n",
    "import os\n",
    "import datetime\n",
    "import pygame\n",
    "import mujoco\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all of our analysis and improvement process of the given PPO implementation on open ai's hopper enviroment. A detailed comparison in respect to implementation details and agent performance between stable baselines PPO and our PPO is included too.\n",
    "\n",
    "**Table of Content:**\n",
    "1) Description of the hopper enviroment\n",
    "2) Training an Agent in the Hopper enviroment with our continuous PPO used in the MountainCarContinuous enviroment\n",
    "3) Training an Agent in the Hopper enviroment with stable baselines 3\n",
    "4) Analysis of differences (performance and implementation)\n",
    "5) Trys to improve our PPO Continuous with findings from the analysis\n",
    "6) Lessons Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Description of the Hopper-v4 enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"visu/hopper-env.jpg\" alt=\"Hopper-v4 Enviroment\" width=\"100\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation Space (11 dimensional):**\n",
    "- Height of the hopper [-Inf … Inf]\n",
    "- Angle of all joints and the top [-Inf … Inf]\n",
    "- Angular velocity of all joints and the top [-Inf … Inf]\n",
    "- Velocity of the top in X and Z of the world [-Inf … Inf]\n",
    "\n",
    "**Action Space (3 dimensional):** \n",
    "- torque applied to the top joint [-1 … 1]\n",
    "- torque applied to the leg joint [-1 … 1]\n",
    "- torque applied to the foot joint [-1 … 1]\n",
    "\n",
    "**Episode End:**\n",
    "- Termination if hopper is unhealthy:\n",
    "    - hopper has fallen (healthy z range)\n",
    "    - Angle of the tigh joint is to big (healthy angle range)\n",
    "    - All other observations are out of range e.g. hopper leaves the enviroment (healthy_state_range)\n",
    "- Truncation if episode step >= 1000\n",
    "\n",
    "**Rewards = sum of:**\n",
    "- Healthy reward (not terminated)\n",
    "- Forward_reward:  positive if hopper hops to the right\n",
    "(forward_reward_weight * (x before action – x after action)/dt\n",
    "- Ctrl_cost: penalizing big actions\n",
    "- Ctrl_cost_weight * sum (action²)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Training of an Agent with our continuous PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The used code was modified from discrete action space to continuous action space on the MountainCar enviroments.\n",
    "\n",
    "To understand the changes were needed to switch the action space to continuous, please refer to the provided MountainCar  notebook.\n",
    "\n",
    "To run this algorithm on hopper enviroment, we must only adapt the train_agent method, because the hoppers behavior in the rollouts (gathering experience from the enviroment).\n",
    "\n",
    "**Differences in the train methods:**\n",
    "- MountainCar and MountainCarContinuous terminate if they reach the goal at top and truncate if a specific count of timesteps was passed.\n",
    "- Hopper terminates if he's unhealthy (see description in 1) or truncates after 1000 passed timesteps.\n",
    "    - because a untrained hopper falls instantly after spawning in the enviroment the rollouts would be very short, if the training method gathers experience for a specific count of rolloust in the enviroment --> no data to learn from\n",
    "\n",
    "--> Our solution was letting the agent gather experience until a limited count of timesteps instead of ending the experience gathering after a limited count of rollouts.\n",
    "\n",
    "For furhter coding details please refer to the train_agent.py mathod."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Initializing and Training </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory (./model)to save all trained agents with a timestamp\n",
    "jetzt = datetime.datetime.now()\n",
    "datum_uhrzeit = jetzt.strftime(\"%Y%m%d_%H%M%S\")\n",
    "savedir = f'model\\\\YOUR_AGENT_NAME_HERE_{datum_uhrzeit}'\n",
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "# create a directory to save logs from Tensorboard (Visualization and analysis tool from Tensorflow)\n",
    "log_dir1 = f\"{savedir}\\\\log\"\n",
    "os.makedirs(log_dir1, exist_ok=True)\n",
    "\n",
    "# user-feedback if logdir already exists --> should not be possible with using the timestamp, \n",
    "# but better prove to avoid overwiriting of the log - data\n",
    "if os.path.exists(log_dir1):\n",
    "    print(f\"The directory {log_dir1} exists.\")\n",
    "    absolute_path = os.path.abspath(log_dir1)\n",
    "    print(absolute_path)\n",
    "else:\n",
    "    print(f\"The directory {log_dir1} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter for the actor and critic networks --> standards used in the MountainCarContinuous\n",
    "actor_learning_rate = 0.00025   # learning rate for the actor\n",
    "critic_learning_rate = 0.001    # learning rate for the critic (shold be > than actor)\n",
    "# If your actor changes faster than your critic, your estimated Q-value will not truly represent the value of your action, because that value is based on the past policies\n",
    "\n",
    "# Parameter for the agent\n",
    "gamma = 0.99                    # discount factor\n",
    "epsilon = 0.2                   # clip range for the actor loss function\n",
    "\n",
    "# Parameter for training (n_rollouts NOT USED)\n",
    "epochs =  50                 # number of learning iterations\n",
    "n_steps = 2048                 # number of steps per epoch -> issue wth hopper while using rollouts\n",
    "batch_size = 16                  # number of samples per learning step\n",
    "learn_steps = 10                # number of learning steps per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the enviroment from gym\n",
    "# Note: possible reward-weights from the hopper-enviroment are all defaults! \n",
    "env = gym.make('Hopper-v4', render_mode='rgb_array')\n",
    "\n",
    "# user-feedback about dimensions of observation and action space\n",
    "print('Observation LOW: ',env.observation_space.low)\n",
    "print('Observation LOW: ',env.observation_space.high)\n",
    "\n",
    "print('Action LOW: ',env.action_space.low)\n",
    "print('Action LOW: ',env.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.PPOAgentContinuous import PPOAgentContinuous as PPOAgent\n",
    "agent = PPOAgent(env.action_space, env.observation_space, gamma, epsilon, actor_learning_rate, critic_learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the agent\n",
    "from lib.train_agent import training_steps\n",
    "training_steps(env, agent, log_dir1, epochs, n_steps, batch_size, learn_steps, render=False)\n",
    "\n",
    "# this method will provide a continuous feedback about the training (e.g. steps, epoch, rollouts, actor loss, critic loss)\n",
    "# to save runtime it's recommendet to set the render-flag to false while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the agent to h5 format\n",
    "filepath_actor = f\"{savedir}\\\\actor.h5\"\n",
    "filepath_critic = f\"{savedir}\\\\critic.h5\"\n",
    "\n",
    "# this method saves the trained weights of actor and critic networks to created folder structure (see obove)\n",
    "agent.save_models(filepath_actor, filepath_critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Reloading and Rendering </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the rendering function (self-written) for the gym framework\n",
    "from lib.render_GUI import render_GUI_GYM\n",
    "\n",
    "# set up the enviroment and load the trained agent from directory\n",
    "# set the render_mode to 'human' to create a external window (pyglet) to render a video of the agent acting in the enviroment\n",
    "render_env = gym.make('Hopper-v4', render_mode = 'human')\n",
    "\n",
    "# set up an agent, where the saved weights loaded in\n",
    "render_agent = PPOAgent(render_env.action_space, render_env.observation_space)\n",
    "render_agent._init_networks()\n",
    "\n",
    "filepath_actor = \"model\\\\YOUR_AGENT_NAME_HERE\\\\actor.h5\"\n",
    "filepath_critic = \"model\\\\YOUR_AGENT_NAME_HERE\\\\critic.h5\"\n",
    "\n",
    "# load the saved weights to the initialized model\n",
    "render_agent.load_models(filepath_actor, filepath_critic)\n",
    "\n",
    "# call the render function \n",
    "# Note: to close this window you have to interrupt this running cell, closing the window with the red x doesn't work\n",
    "render_GUI_GYM(render_env, render_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Example for a trained agent </u>\n",
    "\n",
    "Note: embedded videos are created with *PPO_renderung_video.ipynb*\n",
    "\n",
    "Trained agent with given hyperparameters in the env:  \n",
    "<video width=\"200\" height=\"200\" controls>\n",
    "  <source src=\"visu/Video-standard.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "Learning curve (x-axis = epochs/ y-axis = return):  \n",
    "<img src=\"visu/Learning-curve-standard.jpg\" alt=\"Learning Curve Standard\" width=\"400\"/>\n",
    "\n",
    "Note: Training was interrupted after 10 epochs, because this incorrect behavior, no improvements expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Observations to mention </u>\n",
    "\n",
    "- Standard implementation (from MountainCarContinuous) doesn't work at all.\n",
    "- Policy seems to become verv deterministic (head falls back, instant end of episode).\n",
    "- Agent doesn't gather enough experience to learn jumping or learning is to complex.\n",
    "- Changing of hyperparameters and learning-parameters like batch_size, learn_steps, ... --> no improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Training of an agent with stable baselines 3 PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Initializing, Training and Saving </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stable baselines algorithms (installation of the package recommended)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory (./baselines_model)to save all trained agents with a timestamp\n",
    "jetzt = datetime.datetime.now()\n",
    "datum_uhrzeit = jetzt.strftime(\"%Y%m%d_%H%M%S\")\n",
    "savedir = f'baselines_model\\\\YOUR_AGENT_NAME_HERE_{datum_uhrzeit}'\n",
    "os.makedirs('baselines_model', exist_ok=True)\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "# create a directory to save logs from Tensorboard (Visualization and analysis tool from Tensorflow)\n",
    "log_dir = f\"{savedir}\\\\log\"\n",
    "os.makedirs(log_dir1, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the agent (in stable baselines3 using a vectorized enviroment is prefered)\n",
    "# --> SB3 framework, not compatible with GYM framework\n",
    "vec_env = make_vec_env('Hopper-v4')\n",
    "\n",
    "# set up the agent in baselines framework (used policy, enviroment, optional: log)\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log= log_dir)   \n",
    "# this model provides continuous feedback about the training process when verbose 1 (doesn't effects runtime that much)\n",
    "\n",
    "# call the learn method (training ends, if agent passes 1 Mio timesteps ini total)\n",
    "model.learn(total_timesteps=1000000) \n",
    "\n",
    "# save the model (this will create a data.zip folder containing all model informations)\n",
    "model.save(savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Reloading and Rendering </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the rendering function (self-written) for the sb3 framework/ old gym framework\n",
    "from lib.render_GUI import render_GUI_SB3\n",
    "\n",
    "# define the directory the model is loaded from\n",
    "loaddir = 'baseline_model\\YOUR_AGENT_NAME_HERE\\data.zip'\n",
    "\n",
    "# load the model with sb3 function\n",
    "render_model = PPO.load(loaddir)\n",
    "\n",
    "# initialize a enviroment (must be the same framework as used while training)\n",
    "render_env = make_vec_env('Hopper-v4')\n",
    "\n",
    "# call the render function \n",
    "# Note: to close this window you have to interrupt this running cell, closing the window with the red x doesn't work\n",
    "render_GUI_SB3(render_env, render_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Example for a trained agent </u>\n",
    "\n",
    "Trained agent with default hyperparameters from the BaselinesZoo by DLR in the env:      \n",
    "<video width=\"200\" height=\"200\" controls>\n",
    "  <source src=\"visu/Video-SB3.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "Learning curve (x-axis = total timesteps/ y-axis = return):  \n",
    "<img src=\"visu/Learning-curve-SB3.jpg\" alt=\"Learning Curve SB3\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Observations to mention </u>\n",
    "\n",
    "- stable training progress\n",
    "- good performance in this complex enviroment\n",
    "- head is always vertical\n",
    "- hopper tahes big jumps to the righthand side, sometimes it leaves the enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Analysis of the differences in performance and implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> comparison in performance </u>\n",
    "\n",
    "- our basic implementation can not solve the complex hopper enviroment, SB3 can without any problems\n",
    "- our implementation needs much more runtime to process the same timesteps\n",
    "- our implementation is much more sample-efficient (see MountainCar analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> main differences in implementation </u>\n",
    "\n",
    "- our implementation uses frozen targets, SB3 uses saved variables from the last rollout/ minibatch update\n",
    "- network architecture (both have actor and critic network)\n",
    "    - our implementation: 1 hidden layer with 400 units, 1 with 300\n",
    "    - SB3: 2 hidden layers with 64 units each\n",
    "- diffenent loss definition and optimization (both use Adam optimizer)\n",
    "    - our implementation: Critic Loss only related to advantage estimation (by TD-learning), Actor Loss related to clipped surrogate objective (ratio*advantage)\n",
    "    - SB3: both networks optimized with the same loss definition (combination of policy loss (surrogate objective), entropy loss (entropy current policy), actor loss (advantage))\n",
    "- Policy estimation:\n",
    "    - in our implementation mean and std of gaussian action distribution are outputs of the actor network\n",
    "    - in SB3 only the mean is an output, the std is implememnted as a seperate trainable variable.\n",
    "- SB3 uses standardized observations, we don't\n",
    "\n",
    "We expect that the lack of performance in our implementation is caused by it's simplictiy itself. It is to basic to deal with the high requirements of such a complex enviroment like the hopper. For the MountainCarContinuous it seems to be enough, but it's to basic to deal with the high requirements of such a complex enviroment like the hopper.\n",
    "\n",
    "In particular, we assume that the different loss definitions, lack of frozen networks and standardisation is crucial for good agent performances in complex enviroments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Improve our PPO Continuous implementation with features from SB3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Key changes </u>\n",
    "\n",
    "- Define the trainable log standard deviation (instead of outputting the std by the network). We expect this approach provides more flexibility in shaping the exploration strategy (should support more exploration)\n",
    "- Standardizing observations before they were processed by the networks (networks usually can handle inputs between 0-1 better)\n",
    "- changed loss definition for the actor network (adapted to SB3 approach)\n",
    "- implemented the parameter lambda in critic loss definition (trade of between mean and std in advantage calculation)\n",
    "\n",
    "Combining the networks with their loss definition and replacing the frozen networks was not implemented due to lack of time and particular computing power.\n",
    "\n",
    "For more implementation details, please look up the XXXX_modified files in comparison to the standard ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Initializing, Training and Saving </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory (./model)to save all trained agents with a timestamp\n",
    "jetzt = datetime.datetime.now()\n",
    "datum_uhrzeit = jetzt.strftime(\"%Y%m%d_%H%M%S\")\n",
    "savedir = f'modified_model\\\\YOUR_AGENT_NAME_HERE_{datum_uhrzeit}'\n",
    "os.makedirs('modified_model', exist_ok=True)\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "# create a directory to save logs from Tensorboard (Visualization and analysis tool from Tensorflow)\n",
    "log_dir1 = f\"{savedir}\\\\log\"\n",
    "os.makedirs(log_dir1, exist_ok=True)\n",
    "\n",
    "# user-feedback if logdir already exists --> should not be possible with using the timestamp, \n",
    "# but better prove to avoid overwiriting of the log - data\n",
    "if os.path.exists(log_dir1):\n",
    "    print(f\"The directory {log_dir1} exists.\")\n",
    "    absolute_path = os.path.abspath(log_dir1)\n",
    "    print(absolute_path)\n",
    "else:\n",
    "    print(f\"The directory {log_dir1} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter for the actor and critic networks --> standards used in the MountainCarContinuous\n",
    "actor_learning_rate = 0.00025   # learning rate for the actor\n",
    "critic_learning_rate = 0.001    # learning rate for the critic (shold be > than actor)\n",
    "# If your actor changes faster than your critic, your estimated Q-value will not truly represent the value of your action, because that value is based on the past policies\n",
    "\n",
    "# Parameter for the agent\n",
    "gamma = 0.99                    # discount factor\n",
    "epsilon = 0.2                   # clip range for the actor loss function\n",
    "\n",
    "# Parameter for training (n_rollouts NOT USED)\n",
    "epochs =  50                 # number of learning iterations\n",
    "n_steps = 2048                 # number of steps per epoch -> issue wth hopper while using rollouts\n",
    "batch_size = 16                  # number of samples per learning step\n",
    "learn_steps = 10                # number of learning steps per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modified agent\n",
    "from lib.PPOAgentContinuous_modified import PPOAgentContinuous as PPOAgent_mod\n",
    "agent = PPOAgent_mod(env.action_space, env.observation_space, gamma, epsilon, actor_learning_rate, critic_learning_rate) \n",
    "\n",
    "# train the agent\n",
    "from lib.train_agent import training_steps\n",
    "training_steps(env, agent, log_dir1, epochs, n_steps, batch_size, learn_steps, render=False)\n",
    "\n",
    "# this method will provide a continuous feedback about the training (e.g. steps, epoch, rollouts, actor loss, critic loss)\n",
    "# to save runtime it's recommended to set the render-flag to false while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the agent to h5 format\n",
    "filepath_actor = f\"{savedir}\\\\actor.h5\"\n",
    "filepath_critic = f\"{savedir}\\\\critic.h5\"\n",
    "\n",
    "# this method saves the trained weights of actor and critic networks to created folder structure (see obove)\n",
    "agent.save_models(filepath_actor, filepath_critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Reloading and Rendering </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the rendering function (self-written) for the gym framework\n",
    "from lib.render_GUI import render_GUI_GYM\n",
    "\n",
    "# set up the enviroment and load the trained agent from directory\n",
    "# set the render_mode to 'human' to create a external window (pyglet) to render a video of the agent acting in the enviroment\n",
    "render_env = gym.make('Hopper-v4', render_mode = 'human')\n",
    "\n",
    "# set up an agent, where the saved weights loaded in\n",
    "render_agent_mod = PPOAgent_mod(render_env.action_space, render_env.observation_space)\n",
    "render_agent_mod._init_networks()\n",
    "\n",
    "filepath_actor = \"modified_model\\\\YOUR_AGENT_NAME_HERE\\\\actor.h5\"\n",
    "filepath_critic = \"modified_model\\\\YOUR_AGENT_NAME_HERE\\\\critic.h5\"\n",
    "\n",
    "# load the saved weights to the initialized model\n",
    "render_agent_mod.load_models(filepath_actor, filepath_critic)\n",
    "\n",
    "# call the render function \n",
    "# Note: to close this window you have to interrupt this running cell, closing the window with the red x doesn't work\n",
    "render_GUI_GYM(render_env, render_agent_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Example for a trained agent </u>\n",
    "\n",
    "Trained agent with default hyperparameters from the BaselinesZoo by DLR in the env:     \n",
    "<video width=\"200\" height=\"200\" controls>\n",
    "  <source src=\"visu/Video-modified.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "Learning curve (x-axis = total timesteps/ y-axis = return):  \n",
    "<img src=\"visu/Learning-curve-modified.jpg\" alt=\"Learning Curve modified\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Observations to mention </u>\n",
    "\n",
    "- better performance compared to the standard implementation, but still not on the level of SB3 \n",
    "- Implementation of some SB3 approaches makes the training much more stable\n",
    "- Hopper learns how to jump\n",
    "- Hopper desn't move to the righthand side --> no improvements with more training steps expected due to flattening of the learning curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Lessons Learned (overall for MountainCar and Hopper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conversion of a discrete algorithm to a continuous possibly by changing the policy structure (actor network) and some sampeling methods\n",
    "- Performance depends on small implementation details, not only on the algorithm itself (see hopper)\n",
    "- There are a lot possibilities to implement the same algorithm -> huge variablity\n",
    "- Reward Shaping helps to solve difficult environments (with destructive reward)\n",
    "- NaN Issue - for complex environments (e.g. hopper) some of the actor weights became 0 \n",
    "- Units get “deactivated”, Adam Optimizer fails and outputs a NaN – value\n",
    "“Solved” by using leaky_relu instead of relu as activation of the hidden layers (weight = 0 is much less possible)\n",
    "- Avoid calculating a fraction, use log difference instead  better numerical stability\n",
    "- Standardize observations in experience gathering can be crucial, if actions get out of bounds [-1 … 1] to often\n",
    "- Tuning hyperparameters is only for fine-tuning  using defaults worked out to be always the best\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
